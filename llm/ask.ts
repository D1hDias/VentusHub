
import { LLMClient } from './client';
import { models } from './models';

/**
 * A simple script to ask a question to the Gemini LLM from the command line.
 */
async function askGemini() {
  // Extract the question from command line arguments
  const question = process.argv.slice(2).join(' ');

  if (!question) {
    console.error('‚ùå Por favor, forne√ßa uma pergunta.');
    console.log('Uso: npx ts-node llm/ask.ts "Sua pergunta aqui"');
    process.exit(1);
  }

  console.log('ü§ñ Pensando...');

  try {
    // Initialize the client with the Gemini model
    const client = new LLMClient({
      provider: 'gemini',
      model: models.gemini.pro, // Using the default Gemini Pro model
    });

    // Send the question to the LLM
    const response = await client.chat(question);

    // Print the response
    console.log('\n‚úÖ Resposta do Gemini:\n');
    console.log(response);

  } catch (error) {
    console.error('\nüî• Ocorreu um erro ao consultar a LLM:', error);
    process.exit(1);
  }
}

askGemini();
